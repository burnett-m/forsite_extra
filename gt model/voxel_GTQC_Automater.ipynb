{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import tqdm\n",
    "\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.spatial import ConvexHull\n",
    "import math\n",
    "import random\n",
    "import statistics as stats\n",
    "from itertools import chain\n",
    "from itertools import islice \n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from shapely.geometry import MultiPoint\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import laspy\n",
    "\n",
    "import utils.GapFraction_utils\n",
    "from utils.gtqc_descriptors import extract_geometric_features_in_parallel, process_tree_list\n",
    "from utils.gtqc_utils import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory for the new GT QC boxes that you want to add for the model\n",
    "gtqc_FinalDir = r\"\" # Dir == Directory\n",
    "\n",
    "vaPrefixPattern = \"\" # The Prefix assigned to each GT Box\n",
    "#lazType = \".las\" # The LAZ/LAS type for the files that exist within the GT boxes\n",
    "completedGT = False # True : The GT has been QC'ed and we are reviewing the model / False : The GT has not been QC'ed and we are running the model\n",
    "\n",
    "# Only complete this section if running VA stems that are not in boxes\n",
    "va_GT = True # False :  The data is all in GT boxes for review / True : The data is just in a single shapefile to download the LAZ and clip it.\n",
    "va_merged_stems = r\"\" # VA merged stems shapefile path\n",
    "productionPolys = r\"\" # Production polygons shapefile path\n",
    "s3_lazDir = \"\" # Include the final '/' at the end of the S3 directory path\n",
    "\n",
    "\n",
    "# The directory containing all training data\n",
    "allStems_df_trainingDir = r\"\" # Dir == Directory\n",
    "\n",
    "# Locate the LasToLocalCoords EXE file\n",
    "LasToLocalCoords = r\"\" # Path to LasToLocalCoords.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Preprocessing\n",
    "## Access and prepare new GT stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing LasFromCoords ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# if va_GT == False:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     gtqc_finishedStemCollect(gtqc_FinalDir, vaPrefixPattern, completedGT)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# if va_GT == True:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     va_laz_to_local(va_merged_stems, productionPolys, s3_lazDir, gtqc_FinalDir)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# LASandSHP_prep(gtqc_FinalDir,completedGT)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mLasFromCoords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgtqc_FinalDir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcompletedGT\u001b[49m\u001b[43m,\u001b[49m\u001b[43mLasToLocalCoords\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Forsite\\Intefor\\gt model\\utils\\gtqc_utils.py:196\u001b[0m, in \u001b[0;36mLasFromCoords\u001b[1;34m(gtqc_FinalDir, completedGT, LasToLocalCoords)\u001b[0m\n\u001b[0;32m    193\u001b[0m     folderPaths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(gtqc_FinalDir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoxel\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m folderPaths:\n\u001b[1;32m--> 196\u001b[0m     lasFiles \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder,file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.laz\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    197\u001b[0m     heightThresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Predefined threshold\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m# Produce dictionary with all LAS and SHP files\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if va_GT == False:\n",
    "    gtqc_finishedStemCollect(gtqc_FinalDir, vaPrefixPattern, completedGT)\n",
    "if va_GT == True:\n",
    "    va_laz_to_local(va_merged_stems, productionPolys, s3_lazDir, gtqc_FinalDir)\n",
    "LASandSHP_prep(gtqc_FinalDir,completedGT)\n",
    "LasFromCoords(gtqc_FinalDir,completedGT,LasToLocalCoords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Prepare test model descriptors\n",
    "### Produce voxel descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'completedGT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make sure you add the labels CSV file in the LAS directory, labelling whether it's an approved stem or rejected\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcompletedGT\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     folderPaths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(gtqc_FinalDir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoxel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrejected\u001b[39m\u001b[38;5;124m'\u001b[39m),os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(gtqc_FinalDir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoxel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapproved\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m completedGT \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'completedGT' is not defined"
     ]
    }
   ],
   "source": [
    "# Make sure you add the labels CSV file in the LAS directory, labelling whether it's an approved stem or rejected\n",
    "if completedGT == True:\n",
    "    folderPaths = [os.path.join(gtqc_FinalDir,\"voxel\", 'rejected'),os.path.join(gtqc_FinalDir,\"voxel\", 'approved')]\n",
    "if completedGT == False:\n",
    "    folderPaths = [os.path.join(gtqc_FinalDir,\"voxel\")]\n",
    "\n",
    "for folder in folderPaths: # Iterate through the folders\n",
    "    # Get list of TXT files\n",
    "    txtFiles = []\n",
    "    for root,dirs,_ in os.walk(folder):\n",
    "        for name in dirs:\n",
    "            if 'voxel' in os.path.basename(name):\n",
    "                continue\n",
    "            txtFiles.append(os.path.join(root,name,\".txt\"))\n",
    "\n",
    "    # Process all trees voxel fractions in parallel\n",
    "    flattened_df = process_tree_list(txtFiles, run_in_parallel=False)\n",
    "    print(\"Completed processing all trees\")\n",
    "    print(flattened_df.shape)\n",
    "\n",
    "    # Write all dictionaries to a TXT file\n",
    "    os.makedirs(os.path.join(folder,\"voxel\"),exist_ok=True)\n",
    "\n",
    "    if completedGT == True:\n",
    "        # Get all labels data\n",
    "        labelFiles = [os.path.join(folder,file) for file in os.listdir(os.path.join(folder)) if file.endswith(\".csv\")]\n",
    "        label_list = [pd.read_csv(file) for file in labelFiles]\n",
    "        combined_labels = pd.concat(label_list, ignore_index=True)  # Combine into a single DataFrame\n",
    "        allStems_df = pd.merge(flattened_df,combined_labels,on=\"BOX\",how='inner') # Merge labels with the flattened dataframe\n",
    "    else:\n",
    "        allStems_df = flattened_df\n",
    "    allStems_df.to_csv(os.path.join(folder,\"voxel\",\"stems_NoLAZextracts.csv\"),index=False) # Export the first output to CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAZ Extract Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of LAZ Extract columns\n",
    "laz_extract_columnList = [\"BBox_X\",\"BBox_Y\",\"BBox_Z\",\"Var_X\",\"Var_Y\",\"Var_Z\",\"pDensity\",\"Centroid_X\",\"Centroid_Y\",\"Centroid_Z\",\"Eig_X\",\"Eig_Y\",\"Eig_Z\",\"Int_StD\",\"Int_Avg\",\"Int_Skew\",\"Int_Kurt\",\"CHull_1\",\"CHull_2\",\"CHull_3\",\"CHull_4\",\"CHull_5\",\"CHull_6\",\"CHull_7\",\"CHull_8\",\"CHull_9\",\"CHull_10\",\"NN_1\",\"NN_2\",\"NN_3\",\"NN_4\",\"NN_5\",\"NN_6\",\"NN_7\",\"NN_8\",\"NN_9\",\"NN_10\"]\n",
    "\n",
    "# Make sure you add the labels CSV file in the LAS directory, labelling whether it's an approved stem or rejected\n",
    "if completedGT == True:\n",
    "    folderPaths = [os.path.join(gtqc_FinalDir,\"voxel\", 'rejected'),os.path.join(gtqc_FinalDir,\"voxel\", 'approved')]\n",
    "if completedGT == False:\n",
    "    folderPaths = [os.path.join(gtqc_FinalDir,\"voxel\")]\n",
    "\n",
    "for folder in folderPaths:\n",
    "    # Run this cell if you didn't get lasFiles earlier\n",
    "    lasFiles = [os.path.join(folder,file) for file in os.listdir(folder) if file.endswith(\".las\")]\n",
    "\n",
    "    # Extract features from training data\n",
    "    train_features = []\n",
    "    train_features = extract_geometric_features_in_parallel(lasFiles)\n",
    "\n",
    "    # Add data to the full data frame\n",
    "    laz_extract = pd.DataFrame(train_features,columns=laz_extract_columnList)\n",
    "    lasBox = []\n",
    "    for file in lasFiles: # Add box names column\n",
    "        lasBox.append(file.split(\"\\\\\")[-1].replace(\".las\",\"\"))\n",
    "    #print(lasBox)\n",
    "    laz_extract[\"BOX\"] = lasBox # Add box names column in order of lasFiles\n",
    "    allStems_df = pd.concat([pd.read_csv(os.path.join(folder, \"voxel\",file)) for file in os.listdir(os.path.join(folder,\"voxel\")) if file.startswith(\"stems\")]) # Access all CSVs in directory\n",
    "\n",
    "    allStems_df = pd.merge(allStems_df,laz_extract,on=\"BOX\",how='inner') # Merge with big data frame\n",
    "\n",
    "\n",
    "    allStems_df.to_csv(os.path.join(folder,\"voxel\",\"all_stems.csv\"),index=False) # Final output\n",
    "    # Output to voxel/voxel folder or voxel/approved/voxel folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Collect Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27883, 1068)\n"
     ]
    }
   ],
   "source": [
    "allStems_df_training = pd.concat([pd.read_csv(os.path.join(allStems_df_trainingDir,file)) for file in os.listdir(allStems_df_trainingDir) if file.endswith(\".csv\")], ignore_index=True)#allStems_df#[allStems_df[\"BOX\"].str[-3] == \"d\"]\n",
    "# Rename the column 'Labels' to 'Label'\n",
    "#allStems_df_training.rename(columns={'Labels': 'Label'}, inplace=True)\n",
    "\n",
    "print(f\"{allStems_df_training.shape}\")\n",
    "\n",
    "# Drop columns with NaN values and fill in those values with 0s - There shouldn't really be much of this anymore, except for the convex hull and nearest neighbour columns when there aren't enough points\n",
    "# Fill NaN\n",
    "allStems_df_training.fillna(0,inplace=True)\n",
    "a = list(allStems_df_training.columns[allStems_df_training.isna().any()])\n",
    "allStems_df_training = allStems_df_training.drop(columns=a)\n",
    "allStems_df_training.fillna(0,inplace=True)\n",
    "\n",
    "cols_to_drop = [a for a in list(allStems_df_training.columns) if '(' in a]\n",
    "allStems_df_training = allStems_df_training.drop(cols_to_drop,axis=1)\n",
    "\n",
    "\n",
    "# Convert all cols to numeric\n",
    "# Iterate through each column and convert to numeric (except \"BOX\")\n",
    "for col in allStems_df_training.columns:\n",
    "    if col != \"BOX\":  # Skip the \"BOX\" column\n",
    "        # Convert the column to numeric, coercing errors to NaN (if any)\n",
    "        allStems_df_training[col] = pd.to_numeric(allStems_df_training[col], errors='coerce')\n",
    "\n",
    "allStems_df_training = allStems_df_training.drop([\"BBox_X\",\"BBox_Y\",\"BBox_Z\",\"Centroid_X\",\"Centroid_Y\",\"Centroid_Z\"],axis=1)#,'Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HistGradientBoosting classifier...\n",
      "Results for HistGradientBoosting classifier:\n",
      "The model's accuracy is 77.38%\n",
      "-------------------\n",
      "--------  Rej. App.  Precision\n",
      "Rejected [717 372]    0.66\n",
      "Approved [ 259 1441]    0.85\n",
      "Recall   0.73  0.79\n",
      "-------------------\n",
      "Rejected F1 Score: 0.694\n",
      "Approved F1 Score: 0.820\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "nunique = allStems_df_training.nunique()\n",
    "cols_to_drop = nunique[nunique == 1].index\n",
    "allStems_df_training = allStems_df_training.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Set up for modelling\n",
    "allStems_df_Descriptors = allStems_df_training.drop(columns=[\"BOX\",\"Label\"])\n",
    "allStems_df_Labels = allStems_df_training['Label']\n",
    "allStems_df_BOX= allStems_df_training['BOX']\n",
    "\n",
    "# Scaling the data based on minMax values \n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(allStems_df_Descriptors)\n",
    "allStems_df_Descriptors_scaled = min_max_scaler.transform(allStems_df_Descriptors)\n",
    "\n",
    "# Transform to arrays\n",
    "#train_descriptors = np.array(allStems_df_Descriptors)\n",
    "train_descriptors = np.array(allStems_df_Descriptors_scaled)\n",
    "train_labels = np.array(allStems_df_Labels)\n",
    "\n",
    "# Model\n",
    "testSize = 0.10\n",
    "#svm_classifier = SVC(kernel=\"poly\",probability=True,gamma=0.0001,C=100)\n",
    "\n",
    "classifiers = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(max_iter=1000),\n",
    "    \"XGBoost\": XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "}\n",
    "\n",
    "## UNCOMMENT the following line to copy over the best classifier from the last run\n",
    "classifiers = {\"HistGradientBoosting\": HistGradientBoostingClassifier(max_iter=1000)} # copy over the winner classifier here (do nothing if the last one is the best)\n",
    "\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Training {name} classifier...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_descriptors, train_labels, test_size=testSize, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Results for {name} classifier:\")\n",
    "    evaluate_model(y_test, y_pred)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25251, 1067)\n",
      "The columns are identical between datasets\n",
      "[[0.         0.         0.         ... 0.26183769 0.22297313 0.        ]\n",
      " [0.         0.         0.         ... 0.32218884 0.25245244 0.12444728]\n",
      " [0.         0.         0.         ... 0.2089347  0.30611223 0.22815684]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.15466364 0.14103516 0.10066444]\n",
      " [0.01162791 0.         0.         ... 0.09527896 0.19153069 0.13460381]\n",
      " [0.         0.         0.         ... 0.12739115 0.16027083 0.08213523]]\n"
     ]
    }
   ],
   "source": [
    "# Grab the testing data\n",
    "if completedGT == True:\n",
    "    allStemsFiles = [pd.read_csv(os.path.join(gtqc_FinalDir,\"voxel\",ar,\"voxel\",file)) for ar in [\"approved\",\"rejected\"] for file in os.listdir(os.path.join(gtqc_FinalDir,\"voxel\",ar,\"voxel\")) if file.endswith(\"stems.csv\")]\n",
    "    allStems_df = pd.concat(allStemsFiles)\n",
    "    allStems_testing_Labels = allStems_df['Label']\n",
    "    #testing_labels = np.array(allStems_testing_Labels)\n",
    "else:\n",
    "    allStemsFiles = [pd.read_csv(os.path.join(gtqc_FinalDir,\"voxel\",\"voxel\",file)) for file in os.listdir(os.path.join(gtqc_FinalDir,\"voxel\",\"voxel\")) if file.endswith(\"stems.csv\")]\n",
    "    allStems_df = pd.concat(allStemsFiles)\n",
    "\n",
    "allStems_df = pd.concat(allStemsFiles)\n",
    "print(allStems_df.shape)\n",
    "allStems_testing_BOX= allStems_df['BOX']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "nunique = allStems_df.nunique()\n",
    "cols_to_drop = nunique[nunique == 1].index\n",
    "allStems_df_subset = allStems_df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Get the intersection of column names from both DataFrames\n",
    "common_columns = allStems_df_subset.columns.intersection(allStems_df_Descriptors.columns)\n",
    "# Keep only the columns in df1 that are also in df2\n",
    "allStems_df_subset = allStems_df_subset[common_columns]\n",
    "\n",
    "# Set up for modelling\n",
    "if list(allStems_df_subset.columns) == list(allStems_df_Descriptors.columns):\n",
    "    print(\"The columns are identical between datasets\")\n",
    "else:\n",
    "    print(allStems_df_subset.columns)\n",
    "    print(allStems_df_Descriptors.columns)\n",
    "allStems_testing_Descriptors = allStems_df_subset#.drop(columns=[\"BOX\"])#,\"Labels\"])\n",
    "\n",
    "allStems_testing_Descriptors_scaled = min_max_scaler.transform(allStems_testing_Descriptors) # Use training set minMax values to scale the testing data\n",
    "\n",
    "# Transform to arrays\n",
    "#testing_descriptors = np.array(allStems_testing_Descriptors)\n",
    "testing_descriptors = np.array(allStems_testing_Descriptors_scaled)\n",
    "print(testing_descriptors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 : Implementing the model\n",
    "#### 5a: Test the model results on either the GT Boxes or the testing set put aside from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testSize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(within_thresh_labels)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(within_thresh_labels)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% are correctly called above a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% threshold. Total stems = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(within_thresh_labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(allStems_df_subset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Testing the model on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtestSize\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% set aside from training set ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m     testing_proba \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testSize' is not defined"
     ]
    }
   ],
   "source": [
    "threshold = 0.68\n",
    "\n",
    "if completedGT == True:\n",
    "    print(\"--- Testing the model on completed GT boxes ---\")\n",
    "    print(\"\")\n",
    "    test_proba = clf.predict_proba(testing_descriptors)\n",
    "\n",
    "    within_thresh_dict = dict()\n",
    "    correct_stems = []\n",
    "    incorrect_stems = []\n",
    "    for i, stem_prob in enumerate(test_proba):\n",
    "        if stem_prob[1] > threshold:\n",
    "            within_thresh_dict[f\"STEM{i}\"] = [list(allStems_testing_BOX)[i],list(allStems_testing_Labels)[i]]\n",
    "            if list(allStems_testing_Labels)[i] == 0:\n",
    "                incorrect_stems.append(list(allStems_testing_BOX)[i])\n",
    "            else:\n",
    "                correct_stems.append(list(allStems_testing_BOX)[i])\n",
    "            print(f\"Stem {list(allStems_testing_BOX)[i]} (label: {list(allStems_testing_Labels)[i]}) probabilities: {stem_prob[1]}\")\n",
    "\n",
    "    # Calculate the proportion within a threshold\n",
    "    within_thresh_labels = []\n",
    "    for _,val in within_thresh_dict.items():\n",
    "        within_thresh_labels.append(val[1])\n",
    "\n",
    "    print(f\"Incorrect stems: {incorrect_stems}\")\n",
    "    print(\"\")\n",
    "    print(f\"{sum(within_thresh_labels)/len(within_thresh_labels)*100:.2f}% are correctly called above a {threshold*100:.2f}% threshold. Total stems = {len(within_thresh_labels)} out of {len(allStems_df_subset)}\")\n",
    "else:\n",
    "    print(f\"--- Testing the model on {testSize:.2f}% set aside from training set ---\")\n",
    "    print(\"\")\n",
    "    testing_proba = clf.predict_proba(X_test)\n",
    "\n",
    "    within_thresh_dict = dict()\n",
    "    correct_stems = []\n",
    "    incorrect_stems = []\n",
    "    for i, stem_prob in enumerate(testing_proba):\n",
    "        if stem_prob[1] > threshold:\n",
    "            within_thresh_dict[f\"STEM{i}\"] = [i,list(y_test)[i]]\n",
    "            if list(y_test)[i] == 0:\n",
    "                incorrect_stems.append(i)\n",
    "            else:\n",
    "                correct_stems.append(i)\n",
    "            print(f\"Stem {i} (label: {list(y_test)[i]}) probabilities: {stem_prob[1]}\")\n",
    "\n",
    "    # Calculate the proportion within a threshold\n",
    "    within_thresh_labels = []\n",
    "    for _,val in within_thresh_dict.items():\n",
    "        within_thresh_labels.append(val[1])\n",
    "\n",
    "    print(f\"Incorrect stems: {incorrect_stems}\")\n",
    "    print(\"\")\n",
    "    print(f\"{sum(within_thresh_labels)/len(within_thresh_labels)*100:.2f}% are correctly called above a {threshold*100:.2f}% threshold. Total stems = {len(within_thresh_labels)} out of {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5b: Implement the model on new data\n",
    "##### First test it on a small subset of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m max_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m# The number of stems to select for reviewing\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Read the full shapefile and extract only UNIQUE_ID and STEREO_SPP columns\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m stems_with_uniqueIDandSpecies \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_file(va_merged_stems)\n\u001b[0;32m      7\u001b[0m stems_with_uniqueIDandSpecies\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m stems_with_uniqueIDandSpecies\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m      8\u001b[0m stems_with_uniqueIDandSpecies \u001b[38;5;241m=\u001b[39m stems_with_uniqueIDandSpecies[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNIQUE_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTEREO_SPP\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gpd' is not defined"
     ]
    }
   ],
   "source": [
    "# Use this cell to test the model before implementing it\n",
    "max_num = 100 # The number of stems to select for reviewing\n",
    "\n",
    "\n",
    "# Read the full shapefile and extract only UNIQUE_ID and STEREO_SPP columns\n",
    "stems_with_uniqueIDandSpecies = gpd.read_file(va_merged_stems)\n",
    "stems_with_uniqueIDandSpecies.columns = stems_with_uniqueIDandSpecies.columns.str.upper()\n",
    "stems_with_uniqueIDandSpecies = stems_with_uniqueIDandSpecies[[\"UNIQUE_ID\",\"STEREO_SPP\"]]\n",
    "\n",
    "if completedGT == False:\n",
    "    test_proba = clf.predict_proba(testing_descriptors)\n",
    "    os.makedirs(os.path.join(gtqc_FinalDir,\"model_testing\"),exist_ok=True)\n",
    "\n",
    "    within_thresh_dict = dict()\n",
    "    selected_stems = []\n",
    "    for i, stem_prob in enumerate(test_proba):\n",
    "        if stem_prob[1] > threshold:\n",
    "            within_thresh_dict[f\"STEM{i}\"] = [list(allStems_testing_BOX)[i]]\n",
    "            selected_stems.append({list(allStems_testing_BOX)[i]: stem_prob[1]})\n",
    "            print(f\"Stem {list(allStems_testing_BOX)[i]} (probabilities: {stem_prob[1]})\")\n",
    "    print(f\"Total number of stems selected: {len(selected_stems)} out of {len(allStems_df_subset)} | {len(selected_stems)/len(allStems_df_subset)*100:.2f}%\")\n",
    "\n",
    "    # Create a lookup dictionary from the DataFrame\n",
    "    id_to_species = dict(zip(stems_with_uniqueIDandSpecies['UNIQUE_ID'], stems_with_uniqueIDandSpecies['STEREO_SPP']))\n",
    "    # Build new dictionary using the selected_trees keys\n",
    "    species_dict = {list(stem.keys())[0]: id_to_species.get(list(stem.keys())[0], \"UNKNOWN\") for stem in selected_stems}\n",
    "\n",
    "\n",
    "\n",
    "    # Group stems by species\n",
    "    species_groups = {}\n",
    "    for stem_id, species in species_dict.items():\n",
    "        if species not in species_groups:\n",
    "            species_groups[species] = []\n",
    "        species_groups[species].append(stem_id)\n",
    "\n",
    "    # Randomly sample stems from each species group\n",
    "    sampled_stems = []\n",
    "    for species, stems in species_groups.items():\n",
    "        num_to_sample = min(max_num // len(species_groups), len(stems))\n",
    "        sampled_stems.extend(random.sample(stems, num_to_sample))\n",
    "    sampled_stems_dict = {stem: next((s[stem] for s in selected_stems if stem in s), None) for stem in sampled_stems}\n",
    "    print(sampled_stems_dict)\n",
    "    \n",
    "    #print(f\"Randomly sampled stems across species: {sampled_stems[:20]}\")\n",
    "\n",
    "    with open(os.path.join(gtqc_FinalDir, \"model_testing\", \"_selected_stems.txt\"), \"w\") as f:\n",
    "        for stem, prob in sampled_stems_dict.items():\n",
    "            shutil.copy(\n",
    "                os.path.join(gtqc_FinalDir, \"voxel\", stem + \".las\"),\n",
    "                os.path.join(gtqc_FinalDir, \"model_testing\", stem + \".las\"),\n",
    "            )\n",
    "            f.write(f\"{stem}: {prob}\\n\") \n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------------------------------------------\")\n",
    "# Determine the species distribution of the selected stems\n",
    "\n",
    "\n",
    "#unique_sp = stems_with_uniqueIDandSpecies[\"STEREO_SPP\"].unique()\n",
    "\n",
    "total_counts = stems_with_uniqueIDandSpecies[\"STEREO_SPP\"].value_counts()\n",
    "selected_stems_fromFullSet = stems_with_uniqueIDandSpecies[stems_with_uniqueIDandSpecies[\"UNIQUE_ID\"].isin([list(stem.keys())[0] for stem in selected_stems])]\n",
    "selected_counts = selected_stems_fromFullSet[\"STEREO_SPP\"].value_counts()\n",
    "\n",
    "# Calculate proportions\n",
    "proportions = {}\n",
    "for species in total_counts.index:\n",
    "    total = total_counts[species]\n",
    "    selected = selected_counts.get(species, 0)\n",
    "    proportions[species] = selected / total\n",
    "\n",
    "# Print results\n",
    "print(f\"Proportion of each species included in the selected trees above the {threshold*100}% probability threshold:\")\n",
    "for species, proportion in proportions.items():\n",
    "    print(f\"{species}: {proportion:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then, review those point clouds externally (or by using these snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m x_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Directory containing the LAS files\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m las_dir \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(gtqc_FinalDir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_testing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# List all LAS files in the directory\u001b[39;00m\n\u001b[0;32m      7\u001b[0m las_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(las_dir) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.las\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "x_dim = 30\n",
    "\n",
    "# Directory containing the LAS files\n",
    "las_dir = os.path.join(gtqc_FinalDir,\"model_testing\")\n",
    "\n",
    "# List all LAS files in the directory\n",
    "las_files = [f for f in os.listdir(las_dir) if f.endswith('.las')]\n",
    "\n",
    "# Plot each LAS file\n",
    "for las_file in las_files:\n",
    "    # Read the LAS file\n",
    "    las = laspy.read(os.path.join(las_dir, las_file))\n",
    "    \n",
    "    # Extract the x, y, z coordinates\n",
    "    x = las.x\n",
    "    y = las.y\n",
    "    z = las.z\n",
    "    \n",
    "    # Create a 3D plot with 4 subplots\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    # Different angles for the subplots\n",
    "    angles = [(x_dim, 0), (x_dim, 90), (x_dim, 180), (x_dim, 270)]\n",
    "    \n",
    "    for i, angle in enumerate(angles):\n",
    "        ax = fig.add_subplot(1, 4, i+1, projection='3d')\n",
    "        ax.scatter(x, y, z, c=z, cmap='viridis', marker='.', s=4)\n",
    "        \n",
    "        # Set plot title and labels\n",
    "        ax.set_title(f'{las_file.replace(\".las\",\"\")} (Angle: {angle})')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "        \n",
    "        # Set the viewing angle\n",
    "        ax.view_init(elev=angle[0], azim=angle[1])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### And finally, implement the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'completedGT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcompletedGT\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Use this cell to implement the model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     test_proba \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(testing_descriptors)\n\u001b[0;32m      5\u001b[0m     within_thresh_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'completedGT' is not defined"
     ]
    }
   ],
   "source": [
    "if completedGT == False:\n",
    "    # Use this cell to implement the model\n",
    "    test_proba = clf.predict_proba(testing_descriptors)\n",
    "\n",
    "    within_thresh_dict = dict()\n",
    "    selected_stems = []\n",
    "    for i, stem_prob in enumerate(test_proba):\n",
    "        if stem_prob[1] > threshold:\n",
    "            within_thresh_dict[f\"STEM{i}\"] = [list(allStems_testing_BOX)[i]]\n",
    "            selected_stems.append(list(allStems_testing_BOX)[i])\n",
    "            print(f\"Stem {list(allStems_testing_BOX)[i]} (probabilities: {stem_prob[1]})\")\n",
    "\n",
    "    for stem in selected_stems:\n",
    "        if va_GT == False:\n",
    "            temp_dir = os.path.join(gtqc_FinalDir,stem[0:9],\"trees\")\n",
    "            os.makedirs(os.path.join(temp_dir,\"approved\"),exist_ok=True)\n",
    "            if os.path.isfile(os.path.join(temp_dir,stem+\".las\")):\n",
    "                shutil.move(os.path.join(temp_dir,stem+\".las\"),os.path.join(temp_dir,\"approved\",stem+\".las\"))\n",
    "            print(stem[0:9])\n",
    "\n",
    "            # # Save the list of selected stems into voxel folder\n",
    "            # selected_stems_df = pd.DataFrame(selected_stems,columns=[\"StemID\"])\n",
    "            # selected_stems_df.to_csv(os.path.join(gtqc_FinalDir,\"Selected_Stems.csv\"),index=False)\n",
    "        else:\n",
    "            os.makedirs(os.path.join(gtqc_FinalDir,\"approved\"),exist_ok=True)\n",
    "            shutil.move(os.path.join(gtqc_FinalDir,\"voxel\",stem+\".las\"),os.path.join(gtqc_FinalDir,\"approved\",stem+\".las\"))\n",
    "\n",
    "    # Save the list of selected stems into approved folder\n",
    "    selected_stems_df = pd.DataFrame(selected_stems,columns=[\"StemID\"])\n",
    "    selected_stems_df.to_csv(os.path.join(gtqc_FinalDir,\"Selected_Stems.csv\"),index=False)\n",
    "\n",
    "\n",
    "    print(f\"{len(selected_stems)} were moved above a threshold of {threshold*100:.2f}%. Of a total of {len(allStems_df_subset)}, {len(selected_stems)/len(allStems_df_subset)*100:.2f}% were moved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
